"""
Unit tests for D3 assessment formatter

Tests the assessment reports formatter including issue prioritization,
multiple export formats, and human-readable summary generation.

Acceptance Criteria Tests:
- Human-readable summaries âœ“
- JSON export works âœ“  
- Issue prioritization âœ“
- Markdown formatting âœ“
"""

import json
import os
import tempfile
from datetime import datetime, timezone
from unittest.mock import Mock

import pytest

# Mark entire module as slow for CI optimization
pytestmark = pytest.mark.slow

from d3_assessment.formatter import (
    AssessmentFormatter,
    FormattedIssue,
    IssuePriority,
    ReportFormat,
    format_assessment_report,
    get_issue_summary,
)
from d3_assessment.models import AssessmentResult
from d3_assessment.types import AssessmentStatus, IssueSeverity, IssueType


class TestAssessmentFormatter:
    """Test assessment formatter functionality"""

    @pytest.fixture
    def formatter(self):
        """AssessmentFormatter instance for testing"""
        return AssessmentFormatter()

    @pytest.fixture
    def sample_assessment(self):
        """Create sample assessment result for testing"""
        assessment = Mock(spec=AssessmentResult)
        assessment.assessment_id = "test_assessment_001"
        assessment.url = "https://example.com"
        assessment.business_name = "Test Business"
        assessment.status = AssessmentStatus.COMPLETED
        assessment.created_at = datetime(2025, 6, 9, 12, 0, 0, tzinfo=timezone.utc)

        # PageSpeed data
        assessment.pagespeed_data = {
            "performance_score": 65,
            "core_vitals": {"lcp": 3.2, "fid": 150, "cls": 0.15},
            "opportunities": [
                {"id": "render-blocking-resources", "score": 2.5},
                {"id": "unused-css-rules", "score": 1.8},
            ],
        }

        # Tech stack data
        assessment.tech_stack_data = {
            "cms": "WordPress 5.8",
            "has_analytics": False,
            "has_ssl": True,
            "frameworks": ["jQuery"],
            "analytics": [],
        }

        # LLM insights
        assessment.llm_insights = {
            "insights": [
                {
                    "title": "Missing Call-to-Action",
                    "description": "The homepage lacks a clear call-to-action button for conversions",
                    "category": "conversion",
                    "recommendation": "Add a prominent call-to-action button above the fold",
                },
                {
                    "title": "Poor Mobile Experience",
                    "description": "Website is not optimized for mobile devices",
                    "category": "usability",
                    "recommendation": "Implement responsive design for mobile users",
                },
            ],
            "confidence_score": 0.85,
            "model_used": "gpt-4",
        }

        return assessment

    def test_initialization(self, formatter):
        """Test formatter initialization"""
        assert formatter.severity_weights[IssueSeverity.CRITICAL.value] == 10.0
        assert formatter.severity_weights[IssueSeverity.HIGH.value] == 7.0
        assert formatter.severity_weights[IssueSeverity.MEDIUM.value] == 4.0
        assert formatter.severity_weights[IssueSeverity.LOW.value] == 1.0

        assert formatter.impact_multipliers[IssueType.PERFORMANCE.value] == 1.5
        assert formatter.impact_multipliers[IssueType.SEO.value] == 1.3
        assert formatter.impact_multipliers[IssueType.SECURITY.value] == 1.4

        print("âœ“ Formatter initialization works")

    def test_extract_pagespeed_issues(self, formatter, sample_assessment):
        """Test PageSpeed issue extraction"""
        issues = formatter._extract_pagespeed_issues(sample_assessment.pagespeed_data)

        # Should extract LCP, FID, CLS, and overall performance issues
        assert len(issues) >= 3

        # Check LCP issue
        lcp_issue = next((i for i in issues if "LCP" in i.title), None)
        assert lcp_issue is not None
        assert lcp_issue.severity == IssueSeverity.MEDIUM.value
        assert "3.2s" in lcp_issue.description

        # Check FID issue
        fid_issue = next((i for i in issues if "FID" in i.title), None)
        assert fid_issue is not None
        assert "150ms" in fid_issue.description

        # Check CLS issue
        cls_issue = next((i for i in issues if "CLS" in i.title), None)
        assert cls_issue is not None
        assert "0.15" in cls_issue.description

        # Check overall performance
        perf_issue = next((i for i in issues if "Overall Performance" in i.title), None)
        assert perf_issue is not None
        assert perf_issue.severity == IssueSeverity.HIGH.value  # Score 65 = high (< 70)

        print("âœ“ PageSpeed issue extraction works")

    def test_extract_tech_stack_issues(self, formatter, sample_assessment):
        """Test tech stack issue extraction"""
        issues = formatter._extract_tech_stack_issues(sample_assessment.tech_stack_data)

        # Should find missing analytics
        analytics_issue = next((i for i in issues if "Analytics" in i.title), None)
        assert analytics_issue is not None
        assert analytics_issue.severity == IssueSeverity.HIGH.value
        assert analytics_issue.category == "Marketing"

        # Should find WordPress issue
        wp_issue = next((i for i in issues if "WordPress" in i.title), None)
        assert wp_issue is not None
        assert wp_issue.severity == IssueSeverity.MEDIUM.value
        assert wp_issue.category == "Security"

        # Should NOT find SSL issue (has_ssl = True)
        ssl_issue = next((i for i in issues if "SSL" in i.title), None)
        assert ssl_issue is None

        print("âœ“ Tech stack issue extraction works")

    def test_extract_llm_issues(self, formatter, sample_assessment):
        """Test LLM insights extraction"""
        issues = formatter._extract_llm_issues(sample_assessment.llm_insights)

        assert len(issues) == 2

        # Check call-to-action issue
        cta_issue = next((i for i in issues if "Call-to-Action" in i.title), None)
        assert cta_issue is not None
        assert "conversion" in cta_issue.description.lower()
        assert cta_issue.category == "AI Analysis"
        assert cta_issue.priority == IssuePriority.HIGH  # Contains "conversion"

        # Check mobile issue
        mobile_issue = next((i for i in issues if "Mobile" in i.title), None)
        assert mobile_issue is not None
        assert "mobile" in mobile_issue.description.lower()

        print("âœ“ LLM issue extraction works")

    def test_impact_score_calculation(self, formatter):
        """Test impact score calculation"""
        issue = FormattedIssue(
            title="Test Issue",
            description="Test description",
            severity=IssueSeverity.HIGH.value,
            priority=IssuePriority.HIGH,
            impact_score=0.0,
            recommendation="Test recommendation",
            category="Performance",
        )

        score = formatter._calculate_impact_score(issue)
        expected_score = 7.0 * 1.5  # HIGH severity * Performance multiplier
        assert score == expected_score

        print("âœ“ Impact score calculation works")

    def test_priority_determination(self, formatter):
        """Test priority determination - Issue prioritization"""
        issue_critical = FormattedIssue(
            title="Critical Issue",
            description="Test",
            severity=IssueSeverity.CRITICAL.value,
            priority=IssuePriority.LOW,  # Will be updated
            impact_score=15.0,  # High impact
            recommendation="Test",
            category="Security",
        )

        priority = formatter._determine_priority(issue_critical)
        assert priority == IssuePriority.CRITICAL

        issue_medium = FormattedIssue(
            title="Medium Issue",
            description="Test",
            severity=IssueSeverity.MEDIUM.value,
            priority=IssuePriority.LOW,
            impact_score=5.0,
            recommendation="Test",
            category="Performance",
        )

        priority = formatter._determine_priority(issue_medium)
        assert priority == IssuePriority.MEDIUM

        print("âœ“ Priority determination works")

    def test_extract_and_prioritize_issues(self, formatter, sample_assessment):
        """Test complete issue extraction and prioritization - Issue prioritization"""
        issues = formatter._extract_and_prioritize_issues(sample_assessment)

        # Should have issues from all sources
        assert len(issues) > 5

        # Check that issues are sorted by priority
        priorities = [issue.priority for issue in issues]
        priority_order = [
            IssuePriority.CRITICAL,
            IssuePriority.HIGH,
            IssuePriority.MEDIUM,
            IssuePriority.LOW,
        ]

        for i in range(len(priorities) - 1):
            current_idx = priority_order.index(priorities[i])
            next_idx = priority_order.index(priorities[i + 1])
            assert current_idx <= next_idx, "Issues should be sorted by priority"

        # Check that impact scores are calculated
        for issue in issues:
            assert issue.impact_score > 0

        print("âœ“ Issue extraction and prioritization works")

    def test_generate_summary(self, formatter, sample_assessment):
        """Test human-readable summary generation - Human-readable summaries"""
        top_issues = [
            FormattedIssue(
                title="Critical Performance Issue",
                description="Test",
                severity=IssueSeverity.CRITICAL.value,
                priority=IssuePriority.CRITICAL,
                impact_score=15.0,
                recommendation="Test",
                category="Performance",
            ),
            FormattedIssue(
                title="High Security Issue",
                description="Test",
                severity=IssueSeverity.HIGH.value,
                priority=IssuePriority.HIGH,
                impact_score=10.0,
                recommendation="Test",
                category="Security",
            ),
        ]

        summary = formatter._generate_summary(sample_assessment, top_issues)

        assert isinstance(summary, str)
        assert len(summary) > 100  # Should be substantial
        assert "Test Business" in summary
        assert "critical" in summary.lower()
        assert "performance" in summary.lower() or "user experience" in summary.lower()
        assert "conversion" in summary.lower() or "search ranking" in summary.lower()

        print("âœ“ Human-readable summary generation works")

    def test_format_as_json(self, formatter, sample_assessment):
        """Test JSON formatting - JSON export works"""
        result = formatter.format_assessment(sample_assessment, ReportFormat.JSON)

        assert isinstance(result, dict)

        # Check required fields
        assert "business_name" in result
        assert "website_url" in result
        assert "assessment_date" in result
        assert "overall_score" in result
        assert "summary" in result
        assert "top_issues" in result
        assert "all_issues" in result
        assert "recommendations" in result
        assert "technical_summary" in result
        assert "metadata" in result

        # Check data types
        assert isinstance(result["overall_score"], (int, float))
        assert isinstance(result["top_issues"], list)
        assert isinstance(result["all_issues"], list)
        assert isinstance(result["recommendations"], list)
        assert isinstance(result["technical_summary"], dict)
        assert isinstance(result["metadata"], dict)

        # Test JSON serialization
        json_str = json.dumps(result)
        assert len(json_str) > 100

        print("âœ“ JSON formatting works")

    def test_format_as_markdown(self, formatter, sample_assessment):
        """Test Markdown formatting - Markdown formatting"""
        result = formatter.format_assessment(sample_assessment, ReportFormat.MARKDOWN)

        assert isinstance(result, str)
        assert len(result) > 500  # Should be substantial

        # Check Markdown elements
        assert "# Website Assessment Report" in result
        assert "**Business:**" in result
        assert "**Website:**" in result
        assert "## Executive Summary" in result
        assert "## Priority Issues" in result
        assert "## Action Plan" in result

        # Check business info
        assert "Test Business" in result
        assert "https://example.com" in result

        # Check issue formatting
        assert "ðŸ”´" in result or "ðŸŸ " in result or "ðŸŸ¡" in result  # Priority emojis

        print("âœ“ Markdown formatting works")

    def test_export_to_json(self, formatter, sample_assessment):
        """Test JSON file export - JSON export works"""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
            filepath = f.name

        try:
            success = formatter.export_to_json(sample_assessment, filepath)
            assert success is True

            # Verify file was created and contains valid JSON
            assert os.path.exists(filepath)

            with open(filepath, "r") as f:
                data = json.load(f)

            assert isinstance(data, dict)
            assert "business_name" in data
            assert "website_url" in data
            assert data["business_name"] == "Test Business"

        finally:
            if os.path.exists(filepath):
                os.unlink(filepath)

        print("âœ“ JSON file export works")

    def test_export_to_markdown(self, formatter, sample_assessment):
        """Test Markdown file export"""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".md", delete=False) as f:
            filepath = f.name

        try:
            success = formatter.export_to_markdown(sample_assessment, filepath)
            assert success is True

            # Verify file was created
            assert os.path.exists(filepath)

            with open(filepath, "r") as f:
                content = f.read()

            assert len(content) > 500
            assert "# Website Assessment Report" in content
            assert "Test Business" in content

        finally:
            if os.path.exists(filepath):
                os.unlink(filepath)

        print("âœ“ Markdown file export works")

    def test_get_priority_summary(self, formatter, sample_assessment):
        """Test priority summary generation"""
        summary = formatter.get_priority_summary(sample_assessment)

        assert isinstance(summary, dict)
        assert "total_issues" in summary
        assert "priority_breakdown" in summary
        assert "top_3_issues" in summary
        assert "overall_score" in summary

        # Check priority breakdown
        breakdown = summary["priority_breakdown"]
        assert isinstance(breakdown, dict)
        assert "critical" in breakdown
        assert "high" in breakdown
        assert "medium" in breakdown
        assert "low" in breakdown

        # Check top 3 issues
        top_3 = summary["top_3_issues"]
        assert isinstance(top_3, list)
        assert len(top_3) <= 3

        for issue in top_3:
            assert "title" in issue
            assert "priority" in issue
            assert "category" in issue

        print("âœ“ Priority summary generation works")


class TestUtilityFunctions:
    """Test utility functions"""

    @pytest.fixture
    def sample_assessment(self):
        """Sample assessment for utility tests"""
        assessment = Mock(spec=AssessmentResult)
        assessment.assessment_id = "util_test_001"
        assessment.url = "https://utility-test.com"
        assessment.business_name = "Utility Test Business"
        assessment.status = AssessmentStatus.COMPLETED
        assessment.created_at = datetime.now(timezone.utc)
        assessment.pagespeed_data = {"performance_score": 80}
        assessment.tech_stack_data = {"cms": "WordPress", "has_ssl": True}
        assessment.llm_insights = {"insights": []}
        return assessment

    def test_format_assessment_report_utility(self, sample_assessment):
        """Test format_assessment_report utility function"""
        # Test with default parameters
        result = format_assessment_report(sample_assessment)
        assert isinstance(result, str)
        assert "Website Assessment Report" in result

        # Test with JSON format
        result_json = format_assessment_report(sample_assessment, format_type="json", include_technical=False)
        assert isinstance(result_json, dict)
        assert "business_name" in result_json

        # Test with HTML format
        result_html = format_assessment_report(sample_assessment, format_type="html")
        assert isinstance(result_html, str)
        assert "<html>" in result_html

        print("âœ“ format_assessment_report utility works")

    def test_get_issue_summary_utility(self, sample_assessment):
        """Test get_issue_summary utility function"""
        summary = get_issue_summary(sample_assessment)

        assert isinstance(summary, dict)
        assert "total_issues" in summary
        assert "priority_breakdown" in summary
        assert "top_3_issues" in summary
        assert "overall_score" in summary

        print("âœ“ get_issue_summary utility works")


def test_all_acceptance_criteria():
    """Test that all acceptance criteria are met"""

    acceptance_criteria = {
        "human_readable_summaries": "âœ“ Tested in test_generate_summary and test_format_as_markdown",
        "json_export_works": "âœ“ Tested in test_format_as_json and test_export_to_json",
        "issue_prioritization": "âœ“ Tested in test_priority_determination and test_extract_and_prioritize_issues",
        "markdown_formatting": "âœ“ Tested in test_format_as_markdown and export functionality",
    }

    print("All acceptance criteria covered:")
    for criteria, test_info in acceptance_criteria.items():
        print(f"  - {criteria}: {test_info}")

    assert len(acceptance_criteria) == 4  # All 4 criteria covered
    print("âœ“ All acceptance criteria are tested and working")


if __name__ == "__main__":
    # Run basic functionality test
    import sys

    sys.exit(pytest.main([__file__, "-v"]))
