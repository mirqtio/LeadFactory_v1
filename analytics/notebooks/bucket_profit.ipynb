{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucket Profit Analysis\n",
    "\n",
    "This notebook analyzes profit performance across geo and vertical buckets to identify high-performing segments for campaign optimization.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Geo Buckets**: {affluence}-{density}-{broadband} (e.g., high-urban-fast)\n",
    "- **Vertical Buckets**: {urgency}-{ticket}-{maturity} (e.g., high-mid-growing)\n",
    "- **Goal**: Identify profitable bucket combinations and optimize targeting\n",
    "\n",
    "Phase 0.5 - Task NB-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Color palette for visualizations\n",
    "PALETTE = sns.color_palette(\"husl\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection\n",
    "DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql://user:pass@localhost/leadfactory')\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Cost constants from PRD\n",
    "COST_PER_EMAIL = 0.073  # Total variable cost per email\n",
    "COST_BREAKDOWN = {\n",
    "    'yelp_search': 0.002,\n",
    "    'pagespeed': 0.005,\n",
    "    'openai': 0.015,\n",
    "    'data_axle': 0.050,\n",
    "    'sendgrid': 0.001\n",
    "}\n",
    "\n",
    "print(f\"Connected to database: {DATABASE_URL.split('@')[1] if '@' in DATABASE_URL else 'local'}\")\n",
    "print(f\"Cost per email: ${COST_PER_EMAIL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query bucket performance data\n",
    "bucket_performance_query = \"\"\"\n",
    "SELECT \n",
    "    b.geo_bucket,\n",
    "    b.vert_bucket,\n",
    "    COUNT(DISTINCT e.id) AS emails_sent,\n",
    "    COUNT(DISTINCT CASE WHEN e.opened_at IS NOT NULL THEN e.id END) AS emails_opened,\n",
    "    COUNT(DISTINCT CASE WHEN e.clicked_at IS NOT NULL THEN e.id END) AS emails_clicked,\n",
    "    COUNT(DISTINCT p.id) AS purchases,\n",
    "    SUM(COALESCE(p.price, 0)) AS gross_revenue,\n",
    "    COUNT(DISTINCT e.id) * {cost_per_email} AS total_cost,\n",
    "    SUM(COALESCE(p.price, 0)) - (COUNT(DISTINCT e.id) * {cost_per_email}) AS profit,\n",
    "    CASE \n",
    "        WHEN COUNT(DISTINCT e.id) > 0 \n",
    "        THEN (SUM(COALESCE(p.price, 0)) - (COUNT(DISTINCT e.id) * {cost_per_email})) / COUNT(DISTINCT e.id)\n",
    "        ELSE 0 \n",
    "    END AS profit_per_email\n",
    "FROM businesses b\n",
    "JOIN emails e ON b.id = e.business_id\n",
    "LEFT JOIN purchases p ON b.id = p.business_id\n",
    "WHERE b.geo_bucket IS NOT NULL \n",
    "  AND b.vert_bucket IS NOT NULL\n",
    "  AND e.sent_at >= CURRENT_DATE - INTERVAL '30 days'\n",
    "GROUP BY b.geo_bucket, b.vert_bucket\n",
    "HAVING COUNT(DISTINCT e.id) >= 10\n",
    "ORDER BY profit_per_email DESC;\n",
    "\"\"\".format(cost_per_email=COST_PER_EMAIL)\n",
    "\n",
    "df_buckets = pd.read_sql(bucket_performance_query, engine)\n",
    "print(f\"Loaded {len(df_buckets)} bucket combinations\")\n",
    "df_buckets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query daily profit trends\n",
    "daily_profit_query = \"\"\"\n",
    "SELECT \n",
    "    date_trunc('day', e.sent_at) AS day,\n",
    "    b.geo_bucket,\n",
    "    b.vert_bucket,\n",
    "    COUNT(DISTINCT e.id) AS emails_sent,\n",
    "    COUNT(DISTINCT p.id) AS purchases,\n",
    "    SUM(COALESCE(p.price, 0)) AS gross_revenue,\n",
    "    SUM(COALESCE(f.cost_usd, 0)) AS api_cost,\n",
    "    SUM(COALESCE(p.price, 0)) - SUM(COALESCE(f.cost_usd, 0)) AS profit\n",
    "FROM emails e\n",
    "JOIN businesses b ON e.business_id = b.id\n",
    "LEFT JOIN purchases p ON b.id = p.business_id\n",
    "LEFT JOIN (\n",
    "    SELECT lead_id, SUM(cost_usd) as cost_usd\n",
    "    FROM fct_api_cost\n",
    "    GROUP BY lead_id\n",
    ") f ON e.lead_id = f.lead_id\n",
    "WHERE e.sent_at >= CURRENT_DATE - INTERVAL '30 days'\n",
    "  AND b.geo_bucket IS NOT NULL\n",
    "  AND b.vert_bucket IS NOT NULL\n",
    "GROUP BY 1, 2, 3\n",
    "ORDER BY 1 DESC, profit DESC;\n",
    "\"\"\"\n",
    "\n",
    "df_daily = pd.read_sql(daily_profit_query, engine)\n",
    "print(f\"Loaded {len(df_daily)} daily bucket records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucket Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key metrics\n",
    "if not df_buckets.empty:\n",
    "    # Overall metrics\n",
    "    total_emails = df_buckets['emails_sent'].sum()\n",
    "    total_revenue = df_buckets['gross_revenue'].sum()\n",
    "    total_cost = df_buckets['total_cost'].sum()\n",
    "    total_profit = df_buckets['profit'].sum()\n",
    "    avg_profit_per_email = total_profit / total_emails if total_emails > 0 else 0\n",
    "    \n",
    "    print(\"=== Overall Performance ===\")\n",
    "    print(f\"Total Emails Sent: {total_emails:,}\")\n",
    "    print(f\"Total Revenue: ${total_revenue:,.2f}\")\n",
    "    print(f\"Total Cost: ${total_cost:,.2f}\")\n",
    "    print(f\"Total Profit: ${total_profit:,.2f}\")\n",
    "    print(f\"Avg Profit/Email: ${avg_profit_per_email:.3f}\")\n",
    "    print(f\"\\nROI: {(total_profit/total_cost*100):.1f}%\" if total_cost > 0 else \"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top performing buckets\n",
    "if not df_buckets.empty:\n",
    "    print(\"\\n=== Top 10 Profitable Bucket Combinations ===\")\n",
    "    top_buckets = df_buckets.nlargest(10, 'profit_per_email')[[\n",
    "        'geo_bucket', 'vert_bucket', 'emails_sent', 'purchases', \n",
    "        'gross_revenue', 'profit', 'profit_per_email'\n",
    "    ]]\n",
    "    \n",
    "    # Format currency columns\n",
    "    for col in ['gross_revenue', 'profit', 'profit_per_email']:\n",
    "        top_buckets[col] = top_buckets[col].apply(lambda x: f\"${x:.2f}\")\n",
    "    \n",
    "    display(top_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of profit by bucket combination\n",
    "if not df_buckets.empty:\n",
    "    # Pivot data for heatmap\n",
    "    pivot = df_buckets.pivot_table(\n",
    "        index='geo_bucket',\n",
    "        columns='vert_bucket', \n",
    "        values='profit_per_email',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        pivot, \n",
    "        annot=True, \n",
    "        fmt='.3f',\n",
    "        cmap='RdYlGn',\n",
    "        center=0,\n",
    "        cbar_kws={'label': 'Profit per Email ($)'}\n",
    "    )\n",
    "    plt.title('Profit per Email by Geo and Vertical Bucket', fontsize=16)\n",
    "    plt.xlabel('Vertical Bucket', fontsize=12)\n",
    "    plt.ylabel('Geo Bucket', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion funnel by bucket\n",
    "if not df_buckets.empty:\n",
    "    # Calculate conversion rates\n",
    "    df_buckets['open_rate'] = df_buckets['emails_opened'] / df_buckets['emails_sent']\n",
    "    df_buckets['click_rate'] = df_buckets['emails_clicked'] / df_buckets['emails_sent'] \n",
    "    df_buckets['purchase_rate'] = df_buckets['purchases'] / df_buckets['emails_sent']\n",
    "    \n",
    "    # Top 15 buckets by volume\n",
    "    top_volume = df_buckets.nlargest(15, 'emails_sent')\n",
    "    \n",
    "    # Create funnel visualization\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "    \n",
    "    # Combine bucket names for x-axis\n",
    "    top_volume['bucket_name'] = top_volume['geo_bucket'] + '\\n' + top_volume['vert_bucket']\n",
    "    \n",
    "    # Open rate\n",
    "    axes[0].bar(range(len(top_volume)), top_volume['open_rate'], color=PALETTE[0])\n",
    "    axes[0].set_ylabel('Open Rate', fontsize=12)\n",
    "    axes[0].set_title('Email Performance by Bucket', fontsize=16)\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    \n",
    "    # Click rate  \n",
    "    axes[1].bar(range(len(top_volume)), top_volume['click_rate'], color=PALETTE[1])\n",
    "    axes[1].set_ylabel('Click Rate', fontsize=12)\n",
    "    axes[1].set_ylim(0, 0.5)\n",
    "    \n",
    "    # Purchase rate\n",
    "    axes[2].bar(range(len(top_volume)), top_volume['purchase_rate'], color=PALETTE[2])\n",
    "    axes[2].set_ylabel('Purchase Rate', fontsize=12)\n",
    "    axes[2].set_xlabel('Bucket (Geo / Vertical)', fontsize=12)\n",
    "    axes[2].set_xticks(range(len(top_volume)))\n",
    "    axes[2].set_xticklabels(top_volume['bucket_name'], rotation=45, ha='right')\n",
    "    axes[2].set_ylim(0, 0.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profit trend over time\n",
    "if not df_daily.empty:\n",
    "    # Aggregate by day\n",
    "    daily_totals = df_daily.groupby('day').agg({\n",
    "        'emails_sent': 'sum',\n",
    "        'purchases': 'sum',\n",
    "        'gross_revenue': 'sum',\n",
    "        'api_cost': 'sum',\n",
    "        'profit': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate 7-day rolling average\n",
    "    daily_totals['profit_7d_avg'] = daily_totals['profit'].rolling(7).mean()\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "    \n",
    "    # Daily profit\n",
    "    ax1.plot(daily_totals['day'], daily_totals['profit'], \n",
    "             marker='o', color=PALETTE[0], label='Daily Profit')\n",
    "    ax1.plot(daily_totals['day'], daily_totals['profit_7d_avg'], \n",
    "             linewidth=3, color=PALETTE[1], label='7-Day Average')\n",
    "    ax1.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax1.set_ylabel('Profit ($)', fontsize=12)\n",
    "    ax1.set_title('Daily Profit Trend', fontsize=16)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Volume metrics\n",
    "    ax2.bar(daily_totals['day'], daily_totals['emails_sent'], \n",
    "            alpha=0.5, color=PALETTE[2], label='Emails Sent')\n",
    "    ax2_twin = ax2.twinx()\n",
    "    ax2_twin.plot(daily_totals['day'], daily_totals['purchases'], \n",
    "                  marker='s', color=PALETTE[3], label='Purchases')\n",
    "    \n",
    "    ax2.set_xlabel('Date', fontsize=12)\n",
    "    ax2.set_ylabel('Emails Sent', fontsize=12)\n",
    "    ax2_twin.set_ylabel('Purchases', fontsize=12)\n",
    "    ax2.set_title('Email Volume and Conversions', fontsize=14)\n",
    "    \n",
    "    # Combine legends\n",
    "    lines1, labels1 = ax2.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2_twin.get_legend_handles_labels()\n",
    "    ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucket Profitability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze geo bucket patterns\n",
    "if not df_buckets.empty:\n",
    "    geo_analysis = df_buckets.groupby('geo_bucket').agg({\n",
    "        'emails_sent': 'sum',\n",
    "        'purchases': 'sum',\n",
    "        'gross_revenue': 'sum',\n",
    "        'profit': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    geo_analysis['profit_per_email'] = geo_analysis['profit'] / geo_analysis['emails_sent']\n",
    "    geo_analysis['conversion_rate'] = geo_analysis['purchases'] / geo_analysis['emails_sent']\n",
    "    \n",
    "    # Sort by profit per email\n",
    "    geo_analysis = geo_analysis.sort_values('profit_per_email', ascending=False)\n",
    "    \n",
    "    print(\"=== Geo Bucket Performance ===\")\n",
    "    for _, row in geo_analysis.iterrows():\n",
    "        print(f\"\\n{row['geo_bucket']}:\")\n",
    "        print(f\"  Emails: {row['emails_sent']:,}\")\n",
    "        print(f\"  Profit/Email: ${row['profit_per_email']:.3f}\")\n",
    "        print(f\"  Conversion: {row['conversion_rate']*100:.1f}%\")\n",
    "        print(f\"  Total Profit: ${row['profit']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vertical bucket patterns\n",
    "if not df_buckets.empty:\n",
    "    vert_analysis = df_buckets.groupby('vert_bucket').agg({\n",
    "        'emails_sent': 'sum',\n",
    "        'purchases': 'sum', \n",
    "        'gross_revenue': 'sum',\n",
    "        'profit': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    vert_analysis['profit_per_email'] = vert_analysis['profit'] / vert_analysis['emails_sent']\n",
    "    vert_analysis['conversion_rate'] = vert_analysis['purchases'] / vert_analysis['emails_sent']\n",
    "    vert_analysis['avg_order_value'] = vert_analysis['gross_revenue'] / vert_analysis['purchases']\n",
    "    \n",
    "    # Sort by profit per email\n",
    "    vert_analysis = vert_analysis.sort_values('profit_per_email', ascending=False)\n",
    "    \n",
    "    print(\"\\n=== Vertical Bucket Performance ===\")\n",
    "    for _, row in vert_analysis.iterrows():\n",
    "        print(f\"\\n{row['vert_bucket']}:\")\n",
    "        print(f\"  Emails: {row['emails_sent']:,}\")\n",
    "        print(f\"  Profit/Email: ${row['profit_per_email']:.3f}\")\n",
    "        print(f\"  Conversion: {row['conversion_rate']*100:.1f}%\")\n",
    "        print(f\"  AOV: ${row['avg_order_value']:.2f}\")\n",
    "        print(f\"  Total Profit: ${row['profit']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on analysis\n",
    "if not df_buckets.empty:\n",
    "    print(\"=== Strategic Recommendations ===\")\n",
    "    \n",
    "    # Find most profitable combinations\n",
    "    profitable = df_buckets[df_buckets['profit_per_email'] > 0]\n",
    "    unprofitable = df_buckets[df_buckets['profit_per_email'] <= 0]\n",
    "    \n",
    "    print(f\"\\n1. FOCUS on {len(profitable)} profitable bucket combinations:\")\n",
    "    for _, row in profitable.nlargest(5, 'profit_per_email').iterrows():\n",
    "        print(f\"   • {row['geo_bucket']} + {row['vert_bucket']}: \"\n",
    "              f\"${row['profit_per_email']:.3f}/email\")\n",
    "    \n",
    "    print(f\"\\n2. AVOID {len(unprofitable)} unprofitable combinations\")\n",
    "    \n",
    "    # Volume opportunity\n",
    "    high_profit_low_volume = profitable[\n",
    "        (profitable['profit_per_email'] > profitable['profit_per_email'].median()) &\n",
    "        (profitable['emails_sent'] < profitable['emails_sent'].median())\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n3. SCALE UP {len(high_profit_low_volume)} high-profit, low-volume buckets:\")\n",
    "    for _, row in high_profit_low_volume.nlargest(3, 'profit_per_email').iterrows():\n",
    "        potential = row['profit_per_email'] * 1000  # Potential from 1000 emails\n",
    "        print(f\"   • {row['geo_bucket']} + {row['vert_bucket']}: \"\n",
    "              f\"Potential ${potential:.0f} from 1K emails\")\n",
    "    \n",
    "    # Cost optimization\n",
    "    total_cost_savings = unprofitable['total_cost'].sum()\n",
    "    print(f\"\\n4. COST SAVINGS: ${total_cost_savings:.2f} by stopping unprofitable buckets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export recommendations\n",
    "if not df_buckets.empty:\n",
    "    # Create recommendation dataset\n",
    "    recommendations = df_buckets.copy()\n",
    "    recommendations['recommendation'] = recommendations['profit_per_email'].apply(\n",
    "        lambda x: 'Scale Up' if x > 0.05 else ('Test More' if x > 0 else 'Pause')\n",
    "    )\n",
    "    \n",
    "    # Save to CSV\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'bucket_recommendations_{timestamp}.csv'\n",
    "    recommendations.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"\\nRecommendations exported to: {filename}\")\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(recommendations['recommendation'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Analysis Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze actual API costs if available\n",
    "api_cost_query = \"\"\"\n",
    "SELECT \n",
    "    provider,\n",
    "    COUNT(*) as api_calls,\n",
    "    SUM(cost_usd) as total_cost,\n",
    "    AVG(cost_usd) as avg_cost_per_call,\n",
    "    MIN(cost_usd) as min_cost,\n",
    "    MAX(cost_usd) as max_cost\n",
    "FROM fct_api_cost\n",
    "WHERE timestamp >= CURRENT_DATE - INTERVAL '30 days'\n",
    "GROUP BY provider\n",
    "ORDER BY total_cost DESC;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_costs = pd.read_sql(api_cost_query, engine)\n",
    "    \n",
    "    if not df_costs.empty:\n",
    "        print(\"=== API Cost Breakdown ===\")\n",
    "        total_api_cost = df_costs['total_cost'].sum()\n",
    "        \n",
    "        for _, row in df_costs.iterrows():\n",
    "            pct = (row['total_cost'] / total_api_cost * 100) if total_api_cost > 0 else 0\n",
    "            print(f\"\\n{row['provider']}:\")\n",
    "            print(f\"  Calls: {row['api_calls']:,}\")\n",
    "            print(f\"  Total Cost: ${row['total_cost']:.2f} ({pct:.1f}%)\")\n",
    "            print(f\"  Avg Cost/Call: ${row['avg_cost_per_call']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load API cost data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on this analysis:\n",
    "\n",
    "1. **Immediate Actions**:\n",
    "   - Pause campaigns for unprofitable buckets\n",
    "   - Increase budget allocation to top 5 performing buckets\n",
    "   - A/B test email content for \"Test More\" buckets\n",
    "\n",
    "2. **Data Collection**:\n",
    "   - Gather more samples for low-volume buckets\n",
    "   - Track time-of-day effects on conversions\n",
    "   - Monitor seasonal patterns\n",
    "\n",
    "3. **Model Development**:\n",
    "   - Build predictive model for bucket profitability\n",
    "   - Create budget optimization algorithm\n",
    "   - Develop dynamic pricing by bucket\n",
    "\n",
    "4. **Process Improvements**:\n",
    "   - Automate bucket assignment during enrichment\n",
    "   - Real-time profit tracking dashboard\n",
    "   - Alert system for underperforming buckets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}